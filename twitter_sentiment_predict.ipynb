{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Kris/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Twitter API imports\n",
    "import tweepy as tw\n",
    "\n",
    "# NLP & Sentiment imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download/Update the VADER Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Authentication Verified\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    Authenticates the Alpaca API and Twitter\n",
    "    Returns a pass/fail statement\n",
    "\"\"\"\n",
    "############################################################  \n",
    "    \n",
    "# Setting twitter access and api keys\n",
    "bearer_token = os.getenv(\"TWITTER_BEARER_TOKEN\")\n",
    "consumer_key= os.getenv(\"TWITTER_API_KEY\")\n",
    "consumer_secret= os.getenv(\"TWITTER_SECRET_KEY\")\n",
    "access_token= os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "access_token_secret= os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "\n",
    "# authentication for twitter\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "twitter_api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# test authentication\n",
    "try:\n",
    "    twitter_api.verify_credentials()\n",
    "    auth = \"Twitter Authentication Verified\"\n",
    "except:\n",
    "    auth = \"Error During Twitter Authentication\"\n",
    "    \n",
    "print(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    Twitter: Scrape Tweets and Analyze Sentiment\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def twitter_sentiment(search_word, date_since, items):\n",
    "       \n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # initializing the tweets dataframe\n",
    "    df = []\n",
    "    \n",
    "    # adding retweet filter to search words\n",
    "    search_word = search_word + \" -filter:retweets\"\n",
    "    \n",
    "    # Fetch top tweets/hastags for given ticker\n",
    "    tweets = tw.Cursor(twitter_api.search,\n",
    "              q=search_word,\n",
    "              lang=\"en\",\n",
    "              since=date_since\n",
    "                    ).items(items)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "    \n",
    "        #storing tweet text\n",
    "        tweet_fetched = tweet.text \n",
    "\n",
    "        # Get date of tweet\n",
    "        tweet_date = pd.Timestamp(tweet.created_at, tz=\"America/New_York\").isoformat()\n",
    "        \n",
    "        try:\n",
    "            sentiment = analyzer.polarity_scores(tweet_fetched)\n",
    "            compound = sentiment[\"compound\"]\n",
    "            #pos = sentiment[\"pos\"]\n",
    "            #neu = sentiment[\"neu\"]\n",
    "            #neg = sentiment[\"neg\"]\n",
    "        \n",
    "            df.append({\n",
    "                \"date\": tweet_date,\n",
    "                \"tweet\": tweet_fetched,\n",
    "                \"compound\": compound,\n",
    "                #\"positive\": pos,\n",
    "                #\"negative\": neg,\n",
    "                #\"neutral\": neu\n",
    "            \n",
    "            })\n",
    "        \n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    get_twitter_sentiment(search_words):\n",
    "    Takes 2 argument, word(s) to search tweets for, and number of items (tweets) to return.\n",
    "    \n",
    "    Scrapes Twitter for given search words in tweet\n",
    "    Calculates compound sentiment with VADER sentiment analyzer on each tweet\n",
    "    Calculates average compound sentiment score each 1 hour\n",
    "    Normalizes average hourly VADER compound score\n",
    "    Returns Average Hourly Sentiment Dataframe with Columns: \n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_avg_twitter_sentiment(ticker, search_word):\n",
    "\n",
    "    # tweepy variables\n",
    "    search_word = search_word\n",
    "    date_since = \"2021-01-01\"\n",
    "    items = 1000\n",
    "    twitter_search_phrase = search_word + \" \" + ticker\n",
    "\n",
    "    # call the twitter sentiment function and return a dataframe\n",
    "    tweets_df = twitter_sentiment(twitter_search_phrase, date_since, items)\n",
    "\n",
    "    # Changes the date column to proper datetime format\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "    \n",
    "    # Grouping the tweets by Hour and taking their average Hourly sentiment\n",
    "    avg_hourly_sentiment = tweets_df.groupby(pd.Grouper(key='date', freq='H')).mean().dropna()\n",
    "    \n",
    "    # Get the normalized sentiment score of -1, 0, 1\n",
    "    # avg_hourly_sentiment[\"normalized\"] = avg_hourly_sentiment[\"compound\"].apply(lambda x : get_normalized(x))\n",
    "    \n",
    "    return avg_hourly_sentiment\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Sentiment calculation based on compound score\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_normalized(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.04:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.04:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "############################################################\n",
    "    \"\"\"\n",
    "    This function accepts the column number for the features (X) .\n",
    "    It chunks the data up with a rolling window of Xt - window to predict Xt.\n",
    "    It returns a numpy array of X.\n",
    "    \n",
    "    `df`: The original DataFrame with the time series data.\n",
    "    `window`: The window size in days of previous closing prices that will be used for the prediction.\n",
    "    `feature_col_number`: The column number from the original DataFrame where the features are located.\n",
    "  \n",
    "    \"\"\"\n",
    "############################################################\n",
    "\n",
    "def window_data(df, window, feature_col_number):\n",
    "\n",
    "    X = []\n",
    "    for i in range(len(df) - window):\n",
    "        features = df.iloc[i : (i + window), feature_col_number]\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Machine Learning Model\n",
    "    Input the Sentiment data and return predicted stock returns\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_predictions(combined_df):\n",
    "    \n",
    "    window_size = 1\n",
    "    X = window_data(combined_df, window_size, 0)\n",
    "\n",
    "    # Use the MinMaxScaler to scale data between 0 and 1.\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    # Reshape the features for the model\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    # Load the sentiment model \n",
    "    from tensorflow.keras.models import model_from_json\n",
    "\n",
    "    # load json and create model\n",
    "    file_path = Path(\"ml_model/sentiment_model.json\")\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        sentiment_model_json = json_file.read()\n",
    "    sentiment_loaded_model = model_from_json(sentiment_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    file_path = \"ml_model/sentiment_model.h5\"\n",
    "    sentiment_loaded_model.load_weights(file_path)\n",
    "    \n",
    "    # Make some predictions with the loaded model\n",
    "    predicted = sentiment_loaded_model.predict(X)\n",
    "    \n",
    "    # Recover the original prices instead of the scaled version\n",
    "    predicted_prices = scaler.inverse_transform(predicted)\n",
    "    \n",
    "    # Create a DataFrame of Predicted values\n",
    "    predicted_returns = pd.DataFrame({\n",
    "        \"Predicted\": predicted_prices.ravel()\n",
    "        })\n",
    "\n",
    "    return predicted_returns\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    main_function:\n",
    "    Defines the main function.\n",
    "    1 argument: search word to search twitter and stock prices.\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def tweet_sentiment_main_function(ticker, search_word):\n",
    "      \n",
    "    twitter_sentiment = get_avg_twitter_sentiment(ticker, search_word)\n",
    "    predicted_df_all = get_predictions(twitter_sentiment)\n",
    "    predicted_df = predicted_df_all.head(5)\n",
    "    \n",
    "    return predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='lstm_3_input'), name='lstm_3_input', description=\"created by layer 'lstm_3_input'\"), but it was called on an input with incompatible shape (None, 1, 1).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.226018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.226006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.225919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.226003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.226089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted\n",
       "0   0.226018\n",
       "1   0.226006\n",
       "2   0.225919\n",
       "3   0.226003\n",
       "4   0.226089"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#search_word = \"microsoft\" #search_word\n",
    "#ticker = \"MSFT\" #ticker\n",
    "\n",
    "#predicted_df = tweet_sentiment_main_function(ticker, search_word)\n",
    "#predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
