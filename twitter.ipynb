{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil.parser import parse\n",
    "import alpaca_trade_api as tradeapi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Twitter API imports\n",
    "import tweepy as tw\n",
    "\n",
    "# NLP & Sentiment imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting twitter access and api keys\n",
    "bearer_token = os.getenv(\"TWITTER_BEARER_TOKEN\")\n",
    "consumer_key= os.getenv(\"TWITTER_API_KEY\")\n",
    "consumer_secret= os.getenv(\"TWITTER_SECRET_KEY\")\n",
    "access_token= os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "access_token_secret= os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "alpaca_api = tradeapi.REST(alpaca_api_key, alpaca_secret_key, api_version='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "# authentication for twitter\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "twitter_api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# test authentication\n",
    "try:\n",
    "    twitter_api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Kris/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download/Update the VADER Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    Cleans the tweets and removes the included url\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def remove_url(txt):\n",
    "    \"\"\"Replace URLs found in a text string with nothing \n",
    "    (i.e. it will remove the URL from the string).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : string\n",
    "        A text string that you want to parse and remove urls.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The same txt string with url's removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Sentiment calculation based on compound score\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_normalized(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Function that pulls stock data from a given ticker and timeframe.\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def daily_returns(ticker, timeframe):\n",
    "        \n",
    "    # Set current date and the date from one month ago using the ISO format\n",
    "    current_date = pd.Timestamp.now(tz=\"America/New_York\").isoformat()\n",
    "    past_date = pd.Timestamp(\"2020-12-01 00:00\", tz=\"America/New_York\").isoformat()\n",
    "\n",
    "    # Get 4 weeks worth of historical data for AAPL\n",
    "    df = alpaca_api.get_barset(\n",
    "        ticker,\n",
    "        timeframe,\n",
    "        limit=None,\n",
    "        start=past_date,\n",
    "        end=current_date,\n",
    "        after=None,\n",
    "        until=None,\n",
    "    ).df\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Twitter: Scrape Tweets and Analyze Sentiment\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def twitter_sentiment(search_words, date_since, items):\n",
    "    \n",
    "    # initializing the tweets dataframe\n",
    "    df = []\n",
    "    \n",
    "    # adding retweet filter to search words\n",
    "    search_words = search_words + \" -filter:retweets\"\n",
    "    \n",
    "    # Fetch top tweets/hastags for given ticker\n",
    "    tweets = tw.Cursor(twitter_api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since=date_since\n",
    "                    ).items(items)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "    \n",
    "        #Clean text of tweets\n",
    "        tweet_clean = remove_url(tweet.text)\n",
    "\n",
    "        # Get date of tweet\n",
    "        tweet_date = pd.Timestamp(tweet.created_at, tz=\"America/New_York\").isoformat()\n",
    "        \n",
    "        try:\n",
    "            sentiment = analyzer.polarity_scores(tweet_clean)\n",
    "            compound = sentiment[\"compound\"]\n",
    "            pos = sentiment[\"pos\"]\n",
    "            neu = sentiment[\"neu\"]\n",
    "            neg = sentiment[\"neg\"]\n",
    "        \n",
    "            df.append({\n",
    "                \"date\": tweet_date,\n",
    "                \"tweet\": tweet_clean,\n",
    "                \"compound\": compound,\n",
    "                \"positive\": pos,\n",
    "                \"negative\": neg,\n",
    "                \"neutral\": neu\n",
    "            \n",
    "            })\n",
    "        \n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 166\n",
      "Rate limit reached. Sleeping for: 855\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-d96ffab75cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# call the twitter sentiment function and return a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtwitter_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_since\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Creates datetime object and outputs only the Date and Hour for each tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-420bbd761706>\u001b[0m in \u001b[0;36mtwitter_sentiment\u001b[0;34m(search_words, date_since, items)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     ).items(items)\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#Clean text of tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m                                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                                         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rate limit reached. Sleeping for: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sleep for few extra sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;31m# if self.wait_on_rate_limit and self._reset_time is not None and \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tweepy variables\n",
    "search_words = \"aapl\"\n",
    "date_since = \"2020-10-01\"\n",
    "items = 5000\n",
    "\n",
    "# call the twitter sentiment function and return a dataframe\n",
    "twitter_df = twitter_sentiment(search_words, date_since, items)\n",
    "\n",
    "# Creates datetime object and outputs only the Date and Hour for each tweet\n",
    "twitter_df['date'] = pd.to_datetime(twitter_df['date'])\n",
    "twitter_df['date'] = twitter_df['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%d-%Y %H'))\n",
    "\n",
    "# twitter_df.to_csv(r'C:\\Users\\Kris\\Documents\\ucb_fintech\\Homework\\Project2\\tweets\\aapl_tweets.csv')\n",
    "twitter_df.head()\n",
    "\n",
    "# Grouping the tweets by day and taking their avererage daily sentiment\n",
    "#twitter_df.rename(columns = {'index':'date'}, inplace = True)\n",
    "avg_daily_sentiment = twitter_df.groupby(\"date\").mean()\n",
    "# Fix the average for the negative sentiment score. It is not producing a negative result properly for the normalizing\n",
    "avg_daily_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive stats from the DataFrame\n",
    "twitter_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.plot(\n",
    "    y=[\"compound\", \"positive\", \"negative\", \"neutral\"],\n",
    "    kind=\"line\",\n",
    "    title= \"Twitter Sentiment\",\n",
    "    figsize= (20,10),\n",
    "    grid=True,\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_daily_sentiment[\"normalized\"] = avg_daily_sentiment[\"compound\"].apply(lambda x : get_normalized(x))\n",
    "avg_daily_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca api variables\n",
    "ticker = \"MSFT\"\n",
    "timeframe = \"15Min\"\n",
    "\n",
    "# call the alpaca api and return a dataframe of daily returns\n",
    "daily_df = daily_returns(ticker, timeframe)\n",
    "\n",
    "\n",
    "# Drop Outer Table Level and drop extra columns\n",
    "daily_df = daily_df.droplevel(axis=1, level=0)\n",
    "daily_df = daily_df.drop(columns=[\"open\", \"high\", \"low\", \"volume\"])\n",
    "\n",
    "returns_df = daily_df.pct_change().dropna()\n",
    "\n",
    "# Creates datetime object and outputs only the Date and Hour for each close\n",
    "returns_df.reset_index(inplace = True)\n",
    "returns_df['time'] = returns_df['time'].apply(lambda x: pd.Timestamp(x).strftime('%m-%d-%Y %H'))\n",
    "# is this returning the proper timestamp - should return 01-14??\n",
    "\n",
    "avg_hourly_returns = returns_df.groupby(\"time\").mean()\n",
    "avg_hourly_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = avg_hourly_returns.join(avg_daily_sentiment).dropna(how=\"any\")\n",
    "\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate the twitter sentiment to returns\n",
    "combined_df.corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
