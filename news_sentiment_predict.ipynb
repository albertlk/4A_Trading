{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/albertkong/opt/anaconda3/envs/pyvizenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/albertkong/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Alpaca API imports\n",
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "#News API imports\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "\n",
    "# NLP & Sentiment imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download/Update the VADER Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set News API Key\n",
    "api_key=os.getenv(\"NEWS_API_KEY\")\n",
    "newsapi = NewsApiClient(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    News API: Pull Articles \n",
    "    Use newsapi client to get most relevant \n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_articles(search_word):\n",
    "    \n",
    "    # Set current date and the date from one month ago using the ISO format\n",
    "    now = pd.Timestamp.now(tz=\"America/New_York\").isoformat()\n",
    "    past_date = pd.Timestamp(\"2020-12-21T00:00:00\", tz=\"America/New_York\").isoformat()\n",
    "    \n",
    "    articles = newsapi.get_everything(\n",
    "        q=search_word,\n",
    "        from_param=str(past_date),\n",
    "        to=str(now),\n",
    "        language='en',\n",
    "        sort_by='publishedAt',\n",
    "        )\n",
    "        \n",
    "    return articles\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    News API: Analyze Article Sentiment\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def news_sentiment(news_df):\n",
    "       \n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "     \n",
    "    # Create the sentiment scores DataFrame\n",
    "    sentiments = []\n",
    "\n",
    "    for article in news_df[\"articles\"]:\n",
    "        try:\n",
    "            text = article[\"content\"]\n",
    "            date = pd.to_datetime(article[\"publishedAt\"]) \n",
    "            sentiment = analyzer.polarity_scores(text)\n",
    "            compound = sentiment[\"compound\"]\n",
    "            #pos = sentiment[\"pos\"]\n",
    "            #neu = sentiment[\"neu\"]\n",
    "            #neg = sentiment[\"neg\"]\n",
    "            \n",
    "            sentiments.append({\n",
    "                \"text\": text,\n",
    "                \"date\": date,\n",
    "                \"compound\": compound,\n",
    "                #\"positive\": pos,\n",
    "                #\"negative\": neg,\n",
    "                #\"neutral\": neu\n",
    "                \n",
    "            })\n",
    "            \n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(sentiments)\n",
    "    cols = [\"date\", \"text\", \"compound\"]\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    get_avg__news_sentiment(search_words):\n",
    "    Takes 1 argument, word(s) to search articles for.\n",
    "    \n",
    "    Scrapes articles for given search words \n",
    "    Calculates compound sentiment with VADER sentiment analyzer on each article\n",
    "    Calculates average compound sentiment score each 1 hour\n",
    "    Normalizes average hourly VADER compound score\n",
    "    Returns Average Hourly Sentiment Dataframe with Columns: \n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_avg_news_sentiment(ticker, search_word):\n",
    "   \n",
    "    # fetch articles in dataframe\n",
    "    news = get_articles(search_word + \" AND \" + ticker)\n",
    "\n",
    "    # call the sentiment function and return a dataframe\n",
    "    news_sentiment_df = news_sentiment(news)\n",
    "\n",
    "    # Changes the date column to proper datetime format\n",
    "    # news_sentiment_df['date'] = pd.Timestamp(news_sentiment_df['date'])\n",
    "    # news_sentiment_df.sort_values(by=\"date\", axis=0, ascending=False).dropna()\n",
    "    \n",
    "    # Grouping the tweets by Hour and taking their average Hourly sentiment\n",
    "    avg_hourly_sentiment = news_sentiment_df.groupby(pd.Grouper(key=\"date\", freq='H')).mean().dropna()\n",
    "    \n",
    "    # Get the normalized sentiment score of -1, 0, 1\n",
    "    # avg_hourly_sentiment[\"normalized\"] = avg_hourly_sentiment[\"compound\"].apply(lambda x : get_normalized(x))\n",
    "    \n",
    "    return avg_hourly_sentiment\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Sentiment calculation based on compound score\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_normalized(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.04:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.04:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "############################################################\n",
    "    \"\"\"\n",
    "    This function accepts the column number for the features (X) .\n",
    "    It chunks the data up with a rolling window of Xt - window to predict Xt.\n",
    "    It returns a numpy array of X.\n",
    "    \n",
    "    `df`: The original DataFrame with the time series data.\n",
    "    `window`: The window size in days of previous closing prices that will be used for the prediction.\n",
    "    `feature_col_number`: The column number from the original DataFrame where the features are located.\n",
    "  \n",
    "    \"\"\"\n",
    "############################################################\n",
    "\n",
    "def window_data(df, window, feature_col_number):\n",
    "\n",
    "    X = []\n",
    "    for i in range(len(df) - window):\n",
    "        features = df.iloc[i : (i + window), feature_col_number]\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Machine Learning Model\n",
    "    Input the Sentiment data and return predicted stock returns\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_news_predictions(combined_df):\n",
    "    \n",
    "    window_size = 1\n",
    "    X = window_data(combined_df, window_size, 0)\n",
    "\n",
    "    # Use the MinMaxScaler to scale data between 0 and 1.\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    # Reshape the features for the model\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    # Load the sentiment model \n",
    "    from tensorflow.keras.models import model_from_json\n",
    "\n",
    "    # load json and create model\n",
    "    file_path = Path(\"ml_model/sentiment_model.json\")\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        sentiment_model_json = json_file.read()\n",
    "    sentiment_loaded_model = model_from_json(sentiment_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    file_path = \"ml_model/sentiment_model.h5\"\n",
    "    sentiment_loaded_model.load_weights(file_path)\n",
    "    \n",
    "    # Make some predictions with the loaded model\n",
    "    predicted = sentiment_loaded_model.predict(X)\n",
    "    \n",
    "    # Recover the original prices instead of the scaled version\n",
    "    predicted_prices = scaler.inverse_transform(predicted)\n",
    "    \n",
    "    # Create a DataFrame of Predicted values\n",
    "    predicted_returns = pd.DataFrame({\n",
    "        \"Predicted\": predicted_prices.ravel()\n",
    "        })\n",
    "\n",
    "    return predicted_returns\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    main_function:\n",
    "    Defines the main function.\n",
    "    2 argument: search word to search twitter and stock prices.\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def news_sentiment_main_function(ticker, search_word):\n",
    "      \n",
    "    news_sentiment = get_avg_news_sentiment(ticker, search_word) \n",
    "    predicted_df_all = get_news_predictions(news_sentiment)\n",
    "    predicted_df = predicted_df_all.head(5)\n",
    "    \n",
    "    return predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='lstm_3_input'), name='lstm_3_input', description=\"created by layer 'lstm_3_input'\"), but it was called on an input with incompatible shape (None, 1, 1).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.135716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.135716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.135531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.135622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted\n",
       "0   0.135716\n",
       "1   0.135716\n",
       "2   0.135531\n",
       "3   0.135622\n",
       "4   0.135716"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search_word = \"microsoft\" #search_word\n",
    "# ticker = \"MSFT\" #ticker\n",
    "\n",
    "# predicted_df = news_sentiment_main_function(ticker, search_word)\n",
    "# predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
