{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Kris/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Alpaca API imports\n",
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "#News API imports\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "\n",
    "# NLP & Sentiment imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download/Update the VADER Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    Authenticates the Alpaca API and Twitter\n",
    "    Returns a pass/fail statement\n",
    "\"\"\"\n",
    "############################################################  \n",
    "    \n",
    "# Set News API Key\n",
    "newsapi = NewsApiClient(api_key=os.environ[\"NEWS_API_KEY\"])\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "alpaca_api = tradeapi.REST(alpaca_api_key, alpaca_secret_key, api_version='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    Function that pulls stock data from a given ticker and timeframe.\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def daily_returns(ticker, timeframe):\n",
    "        \n",
    "    # Set current date and the date from one month ago using the ISO format\n",
    "    current_date = pd.Timestamp.now(tz=\"America/New_York\").isoformat()\n",
    "    past_date = pd.Timestamp(\"2021-01-01 00:00\", tz=\"America/New_York\").isoformat()\n",
    "\n",
    "    # Get 4 weeks worth of historical data for AAPL\n",
    "    df = alpaca_api.get_barset(\n",
    "        ticker,\n",
    "        timeframe,\n",
    "        limit=None,\n",
    "        start=past_date,\n",
    "        end=current_date,\n",
    "        after=None,\n",
    "        until=None,\n",
    "    ).df\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    get_ticker_data(ticker):\n",
    "    Takes 1 argument, ticker to search tweets for.\n",
    "    \n",
    "    Scrapes articles for given search words.\n",
    "    Calculates compound sentiment with VADER sentiment analyzer \n",
    "    Normalizes VADER compound score\n",
    "    Returns Average Daily Sentiment Dataframe with Columns: \n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_ticker_data(ticker):\n",
    "    \n",
    "    # alpaca api variables\n",
    "    ticker = ticker\n",
    "    timeframe = \"1D\"\n",
    "\n",
    "    # call the alpaca api and return a dataframe of daily returns\n",
    "    daily_df = daily_returns(ticker, timeframe)\n",
    "\n",
    "    # Drop Outer Table Level and drop extra columns\n",
    "    daily_df = daily_df.droplevel(axis=1, level=0)\n",
    "    daily_df = daily_df.drop(columns=[\"open\", \"high\", \"low\", \"volume\"])\n",
    "\n",
    "    # Get the percent change of the closing prices, drop any NA rows, and reset the index\n",
    "    daily_df[\"percent_change\"] = daily_df.pct_change().dropna()\n",
    "    \n",
    "    # Grouping the tweets by Hour and taking their average Hourly sentiment\n",
    "    avg_returns = daily_df.groupby(pd.Grouper(level=0, freq='H')).mean().dropna()\n",
    "\n",
    "    return daily_df\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    News API: Pull Articles \n",
    "    Use newsapi client to get most relevant \n",
    "    20 headlines per day in the past month\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_articles(keyword):\n",
    "      \n",
    "    # Set current date and the date from one month ago using the ISO format\n",
    "    now = pd.Timestamp.now(tz=\"America/New_York\").isoformat()\n",
    "    past_date = pd.Timestamp(\"2021-01-01 00:00\", tz=\"America/New_York\").isoformat()\n",
    "    \n",
    "    articles = newsapi.get_everything(\n",
    "        q=keyword,\n",
    "        from_param=str(past_date),\n",
    "        to=str(now),\n",
    "        language='en',\n",
    "        sort_by='relevancy',\n",
    "        page=3,\n",
    "        )\n",
    "        \n",
    "    return articles\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    News API: Analyze Article Sentiment\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def news_sentiment(news_df):\n",
    "       \n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "     \n",
    "    # Create the sentiment scores DataFrame\n",
    "    sentiments = []\n",
    "\n",
    "    for article in news_df[\"articles\"]:\n",
    "        try:\n",
    "            text = article[\"content\"]\n",
    "            date = pd.Timestamp(article[\"publishedAt\"], tz=\"America/New_York\").isoformat() \n",
    "            sentiment = analyzer.polarity_scores(text)\n",
    "            compound = sentiment[\"compound\"]\n",
    "            #pos = sentiment[\"pos\"]\n",
    "            #neu = sentiment[\"neu\"]\n",
    "            #neg = sentiment[\"neg\"]\n",
    "            \n",
    "            sentiments.append({\n",
    "                \"text\": text,\n",
    "                \"date\": date,\n",
    "                \"compound\": compound,\n",
    "                #\"positive\": pos,\n",
    "                #\"negative\": neg,\n",
    "                #\"neutral\": neu\n",
    "                \n",
    "            })\n",
    "            \n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(sentiments)\n",
    "    cols = [\"date\", \"text\", \"compound\"]\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    get_news_sentiment(search_words):\n",
    "    Takes 1 argument, word(s) to search articles for.\n",
    "    \n",
    "    Scrapes articles for given search words \n",
    "    Calculates compound sentiment with VADER sentiment analyzer on each article\n",
    "    Calculates average compound sentiment score each 1 hour\n",
    "    Normalizes average hourly VADER compound score\n",
    "    Returns Average Hourly Sentiment Dataframe with Columns: \n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_avg_sentiment(search_words):\n",
    "\n",
    "    # newsapi variables\n",
    "    search_words = search_words\n",
    "    \n",
    "    # fetch articles in dataframe\n",
    "    news = get_articles(search_words)\n",
    "\n",
    "    # call the twitter sentiment function and return a dataframe\n",
    "    news_sentiment_df = news_sentiment(news)\n",
    "\n",
    "    # Changes the date column to proper datetime format\n",
    "    news_sentiment_df['date'] = pd.to_datetime(news_sentiment_df['date'])\n",
    "    \n",
    "    # Grouping the tweets by Hour and taking their average Hourly sentiment\n",
    "    avg_hourly_sentiment = news_sentiment_df.groupby(pd.Grouper(key='date', freq='D')).mean().dropna()\n",
    "    \n",
    "    # Get the normalized sentiment score of -1, 0, 1\n",
    "    avg_hourly_sentiment[\"normalized\"] = avg_hourly_sentiment[\"compound\"].apply(lambda x : get_normalized(x))\n",
    "    \n",
    "    return avg_hourly_sentiment\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Sentiment calculation based on compound score\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_normalized(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.04:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.04:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    combine_sentiment_with_close(avg_hourly_returns, avg_hourly_sentiment):\n",
    "    Takes 2 arguments: the two dataframes to combine\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def combine_sentiment_with_close(avg_hourly_sentiment, avg_hourly_returns):\n",
    "\n",
    "    # Combines the average hourly Twitter sentiment dataframe with the hourly percent change dataframe\n",
    "    combined_df = avg_hourly_returns.join(avg_hourly_sentiment).dropna(how=\"any\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Machine Learning Model\n",
    "    Input the Sentiment data and return predicted stock returns\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_predictions(sentiment_df):\n",
    "\n",
    "\n",
    "    return predicted_df\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    main_function:\n",
    "    Defines the main function.\n",
    "    1 argument: search word to search twitter and stock prices.\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def main_function(news_search_word, ticker):\n",
    "      \n",
    "    news_sentiment = get_avg_sentiment(news_search_word)\n",
    "    percent_change = get_ticker_data(ticker)\n",
    "    combined_df = combine_sentiment_with_close(news_sentiment, percent_change) \n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>percent_change</th>\n",
       "      <th>compound</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-05 05:00:00+00:00</th>\n",
       "      <td>218.01</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.73510</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06 05:00:00+00:00</th>\n",
       "      <td>212.22</td>\n",
       "      <td>-0.026558</td>\n",
       "      <td>0.02090</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-07 05:00:00+00:00</th>\n",
       "      <td>218.29</td>\n",
       "      <td>0.028602</td>\n",
       "      <td>0.57190</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-08 05:00:00+00:00</th>\n",
       "      <td>219.55</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>-0.12500</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-11 05:00:00+00:00</th>\n",
       "      <td>217.50</td>\n",
       "      <td>-0.009337</td>\n",
       "      <td>0.05925</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-12 05:00:00+00:00</th>\n",
       "      <td>214.93</td>\n",
       "      <td>-0.011816</td>\n",
       "      <td>0.18538</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-14 05:00:00+00:00</th>\n",
       "      <td>213.02</td>\n",
       "      <td>-0.015665</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-15 05:00:00+00:00</th>\n",
       "      <td>212.58</td>\n",
       "      <td>-0.002066</td>\n",
       "      <td>0.04930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            close  percent_change  compound  normalized\n",
       "time                                                                   \n",
       "2021-01-05 05:00:00+00:00  218.01        0.000918   0.73510         1.0\n",
       "2021-01-06 05:00:00+00:00  212.22       -0.026558   0.02090         0.0\n",
       "2021-01-07 05:00:00+00:00  218.29        0.028602   0.57190         1.0\n",
       "2021-01-08 05:00:00+00:00  219.55        0.005772  -0.12500        -1.0\n",
       "2021-01-11 05:00:00+00:00  217.50       -0.009337   0.05925         1.0\n",
       "2021-01-12 05:00:00+00:00  214.93       -0.011816   0.18538         1.0\n",
       "2021-01-14 05:00:00+00:00  213.02       -0.015665   0.12500         1.0\n",
       "2021-01-15 05:00:00+00:00  212.58       -0.002066   0.04930         1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_search_word = \"microsoft AND MSFT\" #search_word\n",
    "ticker = \"MSFT\" #ticker\n",
    "\n",
    "combined_df = main_function(news_search_word, ticker)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the sentiment ML model\n",
    "\n",
    "\n",
    "# feed the twitter data into the model\n",
    "\n",
    "\n",
    "# output predicted returns to df\n",
    "\n",
    "\n",
    "# return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
